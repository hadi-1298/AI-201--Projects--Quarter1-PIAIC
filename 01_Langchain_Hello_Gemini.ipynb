{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -q"
      ],
      "metadata": {
        "id": "Lr_Gv8NI1_6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai -q"
      ],
      "metadata": {
        "id": "UgNbZsr_2EfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faumq7hb2Kxn",
        "outputId": "83e217ca-00c9-4d53-9ea1-4d1d04c0f7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l51ZZhTi2nUL",
        "outputId": "aa507742-7469-4d29-9774-16ca39124bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import google.generativeai as genai\n",
        "from langchain.llms.base import LLM\n",
        "from typing import Optional, List, Mapping, Any"
      ],
      "metadata": {
        "id": "toZudFXW25GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iX7lNQqp2_z_",
        "outputId": "48c15faf-4399-42a6-f8cc-9d8380eca450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyCyKeBf1ldH-Uzd1itRxsAKPwmrYSU8HDY'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "if not GEMINI_API_KEY:\n",
        "    raise ValueError(\"API key not found. Make sure 'GOOGLE_API_KEY' is set in userdata.\")\n",
        "\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n"
      ],
      "metadata": {
        "id": "I9NtE7rg3KeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeminiLLM(LLM):\n",
        "    model: str = \"gemini-pro\"\n",
        "    temperature: float = 0.7\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"google_gemini\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        model = genai.GenerativeModel(self.model)\n",
        "        response = model.generate_content(prompt, generation_config={\"temperature\": self.temperature})\n",
        "        return response.text\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\"model\": self.model, \"temperature\": self.temperature}\n",
        "\n"
      ],
      "metadata": {
        "id": "wqFqcYHB56Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = GeminiLLM(model=\"gemini-pro\", temperature=0.7)\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n"
      ],
      "metadata": {
        "id": "CTAAaoq16VMr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf4493e3-717e-4f37-fe93-9ce72bff54f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-0eee168035d3>:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt_template)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is computer?\"\n",
        "response = chain.run({\"question\": question})\n",
        "\n",
        "print(\"Answer:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "u0Q1J6fi58AQ",
        "outputId": "4fa90c7c-cfa3-4c60-a42e-4fe08d2b1239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-d5eb758ec09c>:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = chain.run({\"question\": question})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: A computer is an electronic device that can be programmed to carry out a set of instructions. It is capable of performing a wide range of tasks, including data processing, calculations, and controlling other devices. Computers are used in many different fields, including business, education, scientific research, and entertainment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Experiment with Prompts*"
      ],
      "metadata": {
        "id": "U9SWpsqd7OW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fact_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a knowledgeable assistant. Answer this factual question:\\n{question}\"\n",
        ")\n",
        "story_template = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"You are a active storyteller. Write a short story about the following topic:\\n{topic}\"\n",
        ")\n",
        "\n",
        "question = \"Tell me a short story about a Pakistan.\"\n",
        "response = LLMChain(llm=llm, prompt=story_template).run({\"topic\": question})\n",
        "print(\"Story Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "a1X2XjPmRdpc",
        "outputId": "f7866aaa-a0ba-471e-ca73-fd24894250f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Story Response: Amidst the vibrant tapestry of Pakistan, where mountains kiss the skies and ancient rivers whisper tales of yore, there lived a young woman named Amina. With her piercing eyes and a heart as warm as the summer sun, she was a beacon of hope in her bustling hometown of Lahore.\n",
            "\n",
            "One fateful day, as Amina strolled through the bustling markets, her gaze fell upon a group of children huddled in a corner. Their faces were etched with hunger, their eyes filled with desperation. A wave of compassion surged through her, and she knew she had to help.\n",
            "\n",
            "With a heavy heart, Amina approached the children and gently asked about their plight. They explained that they had lost their parents to the cruel hand of fate and were now living on the streets, begging for scraps of food. Amina's resolve solidified—she would not let these innocent souls suffer.\n",
            "\n",
            "Determined to make a difference, Amina sought out the local orphanage. She offered her services as a volunteer, spending countless hours caring for the children, feeding them, and sharing stories that filled their hearts with joy. As days turned into weeks, Amina's presence became a beacon of light in the orphanage, bringing laughter and hope to its young inhabitants.\n",
            "\n",
            "However, Amina's selfless acts did not go unnoticed. News of her kindness spread throughout the community, inspiring others to follow her example. One by one, volunteers joined the orphanage, eager to lend a helping hand. Together, they created a vibrant and nurturing environment where the children could thrive.\n",
            "\n",
            "As the years passed, Amina's legacy lived on. The orphanage she had helped to establish became a sanctuary for countless orphans, providing them with a safe and loving home. Her unwavering compassion had ignited a chain reaction of kindness, transforming the lives of both the children and the community at large.\n",
            "\n",
            "And so, Amina, the young woman from Pakistan, became a symbol of hope and inspiration. Her story serves as a timeless reminder that even the smallest acts of kindness can make a world of difference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Add Money :-***\n",
        "\n"
      ],
      "metadata": {
        "id": "8avd72hiSYiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-experimental -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD30SuR-TH3q",
        "outputId": "4bfa8a43-07e5-4dcc-9550-9792a412b652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m204.8/209.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/2.5 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "response1 = conversation.run(\"What is Artifical Intelligence\")\n",
        "print(\"Q1:\", response1)\n",
        "\n",
        "response2 = conversation.run(\"Can you explain it in simple way?\")\n",
        "print(\"Q2:\", response2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        },
        "id": "rHIbpcBkTIvG",
        "outputId": "9a361ed1-b4a2-4543-e835-180156d2b057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-d2986637d809>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "<ipython-input-11-d2986637d809>:6: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1: Artificial Intelligence (AI) is the emerging field of study that focuses on developing intelligent agents, which are systems that can reason, learn, and act autonomously. AI has a wide range of applications, including:\n",
            "\n",
            "* Natural language processing\n",
            "* Speech recognition\n",
            "* Image recognition\n",
            "* Machine learning\n",
            "* Robotics\n",
            "* Expert systems\n",
            "* Decision support systems\n",
            "* Planning and scheduling\n",
            "* Game playing\n",
            "* Automated reasoning\n",
            "\n",
            "AI is still a relatively new field, but it has already had a significant impact on our world. AI-powered systems are used in a variety of applications, from self-driving cars to medical diagnosis. As AI continues to develop, it is likely to have an even greater impact on our lives.\n",
            "Q2: Sure. Artificial Intelligence (AI) is the field of computer science that gives computers the ability to think and learn like humans. AI-powered systems can perform tasks that normally require human intelligence, such as:\n",
            "\n",
            "* Understanding natural language\n",
            "* Recognizing speech\n",
            "* Identifying objects in images\n",
            "* Making decisions\n",
            "* Solving problems\n",
            "* Playing games\n",
            "\n",
            "AI has a wide range of applications, including:\n",
            "\n",
            "* Self-driving cars\n",
            "* Medical diagnosis\n",
            "* Customer service\n",
            "* Fraud detection\n",
            "* Financial trading\n",
            "* Manufacturing\n",
            "* Robotics\n",
            "\n",
            "AI is still a relatively new field, but it is already having a significant impact on our world. As AI continues to develop, it is likely to have an even greater impact on our lives.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Explore Gemini Features:-***"
      ],
      "metadata": {
        "id": "BGS8OvceWy_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust model settings\n",
        "llm = GeminiLLM(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=userdata.get(\"GOOGLE_API_KEY\"),\n",
        "    temperature=0.9,  # More creative\n",
        "    max_output_tokens=200  # Limit output length\n",
        ")"
      ],
      "metadata": {
        "id": "kALpMSraXDCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Integrate Tools:-***"
      ],
      "metadata": {
        "id": "hQWO-CrGU70r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = GeminiLLM(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=userdata.get(\"GOOGLE_API_KEY\"),\n",
        "    temperature=0.7,\n",
        "    max_output_tokens=150\n",
        ")\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer this question:\\n{question}\"\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "conversation = LLMChain(llm=llm, prompt=template, memory=memory)\n"
      ],
      "metadata": {
        "id": "NEGwjyyjVDkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = conversation.run({\"question\": \"from which year they start study of ai?.\"})\n",
        "print(response2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iZK2m_h7V3K0",
        "outputId": "85dae42a-251c-4278-a788-880cd9466b4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response1 = conversation.run({\"question\": \"who is the founder of ai?\"})\n",
        "print(response1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "Ke3tvIFxVoeS",
        "outputId": "c796e69c-b285-4a28-bf62-e58aafb990cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is no single founder of AI, as it is a field that has been developed by many researchers over time. However, some of the key pioneers in the field include:\n",
            "\n",
            "* **John McCarthy:** Coined the term \"artificial intelligence\" in 1956 and is considered one of the founding fathers of the field.\n",
            "* **Marvin Minsky:** Co-founded the MIT Artificial Intelligence Laboratory and made significant contributions to machine learning and robotics.\n",
            "* **Allen Newell and Herbert Simon:** Developed the first AI program, the Logic Theorist, in 1956.\n",
            "* **Arthur Samuel:** Developed the first self-learning checkers program in 1959.\n",
            "* **Geoffrey Hinton:** A pioneer in deep learning and neural networks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response3 = conversation.run({\"question\": \"which is the powerful aai tool?\"})\n",
        "print(response3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wBDsvVQNWHM4",
        "outputId": "5d47d1d3-458b-406b-d550-719f97fcb710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-3 (Generative Pre-trained Transformer 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response4 = conversation.run({\"question\": \"how GPT-3 is powerful?\"})\n",
        "print(response4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "VeXHbQEfWVDt",
        "outputId": "c37adb3c-6bcc-4ee9-aac3-f9d27dae9047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**GPT-3's Power Stems from:**\n",
            "\n",
            "**1. Massive Size and Training Data:**\n",
            "* Trained on a colossal dataset of over 175 billion parameters.\n",
            "* This vast data provides GPT-3 with a deep understanding of language and its nuances.\n",
            "\n",
            "**2. Transformer Architecture:**\n",
            "* Employs a transformer neural network architecture, which allows it to process and generate sequences of text in parallel.\n",
            "* This enables efficient handling of long-form text and complex language tasks.\n",
            "\n",
            "**3. Generative Capabilities:**\n",
            "* Can generate human-like text, code, and other content.\n",
            "* This makes it useful for a wide range of applications, including natural language processing, content creation, and dialogue generation.\n",
            "\n",
            "**4. Pre-Training and Fine-Tuning:**\n",
            "* Pre-trained on a massive dataset and then fine-tuned on specific tasks.\n",
            "* This allows GPT-3 to adapt to new domains and perform specialized tasks effectively.\n",
            "\n",
            "**5. Contextual Understanding:**\n",
            "* Understands the context and relationships within text.\n",
            "* This enables it to perform tasks such as text summarization, question answering, and fact-checking.\n",
            "\n",
            "**6. Flexibility and Versatility:**\n",
            "* Can be applied to a diverse range of tasks, including:\n",
            "    * Text generation\n",
            "    * Language translation\n",
            "    * Dialogue systems\n",
            "    * Code generation\n",
            "    * Question answering\n",
            "\n",
            "**7. Scalability and Efficiency:**\n",
            "* Trained on a distributed network of GPUs, allowing for efficient handling of large datasets.\n",
            "* This scalability enables GPT-3 to be used for real-time applications and large-scale projects.\n",
            "\n",
            "**8. Learning and Adaptation:**\n",
            "* Can continue to learn and improve over time through further training and fine-tuning.\n",
            "* This ensures its performance remains cutting-edge and adapts to evolving language patterns.\n"
          ]
        }
      ]
    }
  ]
}